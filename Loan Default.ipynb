{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hoRBStIII-y9"
   },
   "source": [
    "\n",
    "Preprocessing includes separating the features (input) and target (output) variables and identifying categorical and numerical columns.\n",
    "Categorical variables are one-hot encoded, and numerical features are standardized using StandardScaler.\n",
    "The dataset is split into training, validation, and test sets for model evaluation.\n",
    "\n",
    "ANN Model Definition:\n",
    "A function build_model(hp) is defined to create a neural network using two hidden layers.\n",
    "Hyperparameters such as the number of units in each layer, activation function, and learning rate are tuned dynamically through Keras Tuner.\n",
    "The model is compiled using the Adam optimizer with a binary cross-entropy loss function and accuracy as a performance metric.\n",
    "\n",
    "Hyperparameter Tuning:\n",
    "\n",
    "The program leverages different hyperparameter optimization techniques, such as Random Search, Bayesian Optimization, and Hyperband, to explore the hyperparameter space and find the best model.\n",
    "The model is evaluated on a test set after tuning to assess its performance.\n",
    "\n",
    "Performance Evaluation:\n",
    "\n",
    "The evaluate_model function is used to assess the model on the test set by calculating the test loss.\n",
    "Results for different optimization techniques, such as test loss and time taken, are collected and displayed in a table for comparison. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values in person_gender: ['female' 'male']\n",
      "Unique values in person_education: ['Master' 'High School' 'Bachelor' 'Associate' 'Doctorate']\n",
      "Unique values in person_home_ownership: ['RENT' 'OWN' 'MORTGAGE' 'OTHER']\n",
      "Unique values in loan_intent: ['PERSONAL' 'EDUCATION' 'MEDICAL' 'VENTURE' 'HOMEIMPROVEMENT'\n",
      " 'DEBTCONSOLIDATION']\n",
      "Unique values in previous_loan_defaults_on_file: ['No' 'Yes']\n"
     ]
    }
   ],
   "source": [
    "# keras-tuner\n",
    "!pip install keras-tuner -q\n",
    "\n",
    "#necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, Adagrad\n",
    "from keras_tuner import RandomSearch, BayesianOptimization, Hyperband\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import os\n",
    "os.chdir(r\"C:\\Users\\hp\\OneDrive - The University of Texas at Dallas\\Financial Institutions\")\n",
    "data = pd.read_csv(\"loan_data.csv\")\n",
    "\n",
    "# List of the categorical columns \n",
    "categorical_cols = ['person_gender', 'person_education', 'person_home_ownership', 'loan_intent', 'previous_loan_defaults_on_file']\n",
    "\n",
    "# unique values for each specified categorical column\n",
    "for col in categorical_cols:\n",
    "    print(f\"Unique values in {col}: {data[col].unique()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing: Separate features and target variable\n",
    "\n",
    "X = data.drop(columns=['loan_status'])\n",
    "y = data['loan_status']\n",
    "\n",
    "# Separate categorical and numerical features\n",
    "categorical_features = [\"person_gender\", \"person_home_ownership\", \"loan_intent\"]\n",
    "label_encoded_features = [\"person_education\", \"previous_loan_defaults_on_file\"]\n",
    "\n",
    "# Label encode ordinal variables separately\n",
    "for col in label_encoded_features:\n",
    "    le = LabelEncoder()\n",
    "    X[col] = le.fit_transform(X[col])\n",
    "\n",
    "# Identify numerical features (excluding categorical + label encoded ones)\n",
    "numerical_features = X.drop(columns=categorical_features + label_encoded_features).columns.tolist()\n",
    "\n",
    "# ColumnTransformer: One hot encoding & label encoded passthrough\n",
    "data_transformer = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_features),\n",
    "        ('cat', OneHotEncoder(drop='first'), categorical_features)  # One-Hot Encode nominal features\n",
    "    ],\n",
    "    remainder='passthrough'  # Keeping label-encoded columns as they are\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "M01a-jNoi2Zu"
   },
   "outputs": [],
   "source": [
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Split into training (60%) and temp (40%)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "\n",
    "# Split temp data into 50% validation, 50% test (which results in 20% validation, 20% test of full data)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Apply preprocessing (only fit on training data)\n",
    "X_train_scaled = data_transformer.fit_transform(X_train)  # Fit only on training data\n",
    "X_val_scaled = data_transformer.transform(X_val)          # Transform validation data\n",
    "X_test_scaled = data_transformer.transform(X_test)        # Transform test data\n",
    "\n",
    "# model-building function for tuning with two hidden layers and added 'sigmoid' as an activation choice\n",
    "#The function must be defined to dynamically create the neural network model with different hyperparameters.\n",
    "#The function takes hp (hyperparameter object) as input and builds the model based on the values provided during each trial.\n",
    "#The function returns a compiled model, which Keras Tuner uses for training and evaluation.\n",
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "\n",
    "    # First hidden layer\n",
    "    model.add(Dense(units=hp.Int('units_0', min_value=32, max_value=128, step=32),\n",
    "                    activation=hp.Choice('activation', values=['relu', 'tanh', 'sigmoid']),\n",
    "                    input_shape=(X_train_scaled.shape[1],)))\n",
    "\n",
    "    # Second hidden layer\n",
    "    model.add(Dense(units=hp.Int('units_1', min_value=32, max_value=128, step=32),\n",
    "                    activation=hp.Choice('activation', values=['relu', 'tanh', 'sigmoid'])))\n",
    "\n",
    "    # Output layer\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "     # Choose optimizer dynamically\n",
    "    optimizer_choice = hp.Choice('optimizer', values=['adam', 'adagrad', 'rmsprop'])\n",
    "\n",
    "    # Choose learning rate dynamically\n",
    "    learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
    "\n",
    "    # Assign the chosen optimizer with the selected learning rate\n",
    "    if optimizer_choice == 'adam':\n",
    "        optimizer = Adam(learning_rate=learning_rate)\n",
    "    elif optimizer_choice == 'adagrad':\n",
    "        optimizer = Adagrad(learning_rate=learning_rate)\n",
    "    else:\n",
    "        optimizer = RMSprop(learning_rate=learning_rate)\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "# Function to evaluate model on the test set and return loss\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    test_loss = model.evaluate(X_test, y_test, verbose=0)[0]  # Return only test loss\n",
    "    return test_loss\n",
    "\n",
    "# Set number of trials (consistent across all methods)\n",
    "max_trials = 10\n",
    "max_epochs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F8jmcc_Ca0sI",
    "outputId": "c32e9c56-fd5e-46fe-a6cf-0ef0b47e4c75"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From E:\\Anaconda\\Lib\\site-packages\\keras\\src\\backend\\common\\global_state.py:82: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Random Search with overwrite=True to force re-run\n",
    "random_tuner = RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_loss',\n",
    "    max_trials=max_trials,\n",
    "    directory='random_search',\n",
    "    project_name='customer_churn',\n",
    "    overwrite=True\n",
    ")\n",
    "\n",
    "start_time = time.time()\n",
    "#The search() method of Keras Tuner starts the hyperparameter tuning process, where it tries different combinations of hyperparameters\n",
    "#and trains the model with those values.\n",
    "random_tuner.search(X_train_scaled, y_train, epochs=max_epochs, validation_data=(X_val_scaled, y_val), verbose=0)\n",
    "random_search_time = time.time() - start_time\n",
    "\n",
    "# Get best model and evaluate on the test set\n",
    "random_search_best_model = random_tuner.get_best_models(num_models=1)[0]\n",
    "random_search_test_loss = evaluate_model(random_search_best_model, X_test_scaled, y_test)\n",
    "random_search_best_hyperparameters = random_tuner.get_best_hyperparameters(num_trials=1)[0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "eE-QlhsOa49N"
   },
   "outputs": [],
   "source": [
    "# Bayesian Optimization with overwrite=True\n",
    "bayesian_tuner = BayesianOptimization(\n",
    "    build_model,\n",
    "    objective='val_loss',\n",
    "    max_trials=max_trials,\n",
    "    directory='bayesian_opt',\n",
    "    project_name='customer_churn',\n",
    "    overwrite=True\n",
    ")\n",
    "\n",
    "start_time = time.time()\n",
    "bayesian_tuner.search(X_train_scaled, y_train, epochs=max_epochs, validation_data=(X_val_scaled, y_val), verbose=0)\n",
    "bayesian_search_time = time.time() - start_time\n",
    "\n",
    "# Get best model and evaluate on the test set\n",
    "bayesian_search_best_model = bayesian_tuner.get_best_models(num_models=1)[0]\n",
    "bayesian_search_test_loss = evaluate_model(bayesian_search_best_model, X_test_scaled, y_test)\n",
    "bayesian_search_best_hyperparameters = bayesian_tuner.get_best_hyperparameters(num_trials=1)[0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "vzLXQ-THag7i"
   },
   "outputs": [],
   "source": [
    "# Hyperband with overwrite=True\n",
    "hyperband_tuner = Hyperband(\n",
    "    build_model,\n",
    "    objective='val_loss',\n",
    "    max_epochs=max_epochs,\n",
    "    directory='hyperband',\n",
    "    project_name='customer_churn',\n",
    "    overwrite=True\n",
    ")\n",
    "\n",
    "start_time = time.time()\n",
    "hyperband_tuner.search(X_train_scaled, y_train, validation_data=(X_val_scaled, y_val), verbose=0)\n",
    "hyperband_search_time = time.time() - start_time\n",
    "\n",
    "# Get best model and evaluate on the test set\n",
    "hyperband_search_best_model = hyperband_tuner.get_best_models(num_models=1)[0]\n",
    "hyperband_search_test_loss = evaluate_model(hyperband_search_best_model, X_test_scaled, y_test)\n",
    "hyperband_search_best_hyperparameters = hyperband_tuner.get_best_hyperparameters(num_trials=1)[0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f0azVtYza_Pa",
    "outputId": "ff48ab33-d4c3-441b-c2f2-0b0ab639752c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Method  Test Loss  Time (seconds)  \\\n",
      "0          Random Search   0.202476      152.223099   \n",
      "1  Bayesian Optimization   0.198675      143.298021   \n",
      "2              Hyperband   0.197792       57.601464   \n",
      "\n",
      "                                                                                                                                                                                                Best Hyperparameters  \n",
      "0                                                                                                                {'units_0': 32, 'activation': 'tanh', 'units_1': 32, 'optimizer': 'rmsprop', 'learning_rate': 0.01}  \n",
      "1                                                                                                            {'units_0': 128, 'activation': 'sigmoid', 'units_1': 32, 'optimizer': 'rmsprop', 'learning_rate': 0.01}  \n",
      "2  {'units_0': 128, 'activation': 'relu', 'units_1': 64, 'optimizer': 'rmsprop', 'learning_rate': 0.01, 'tuner/epochs': 3, 'tuner/initial_epoch': 1, 'tuner/bracket': 1, 'tuner/round': 1, 'tuner/trial_id': '0000'}  \n"
     ]
    }
   ],
   "source": [
    "# Adjust Pandas setting to display full DataFrame content\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# Collecting the results\n",
    "results = {\n",
    "    \"Method\": [\"Random Search\", \"Bayesian Optimization\", \"Hyperband\"],\n",
    "    \"Test Loss\": [random_search_test_loss, bayesian_search_test_loss, hyperband_search_test_loss],\n",
    "    \"Time (seconds)\": [random_search_time, bayesian_search_time, hyperband_search_time],\n",
    "    \"Best Hyperparameters\": [\n",
    "        random_search_best_hyperparameters,\n",
    "        bayesian_search_best_hyperparameters,\n",
    "        hyperband_search_best_hyperparameters\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Creating a DataFrame to display the results\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IqKaOaDcmBZM",
    "outputId": "7d8c823a-081c-40c1-a40c-531dac478d4e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results summary\n",
      "Results in bayesian_opt\\customer_churn\n",
      "Showing 10 best trials\n",
      "Objective(name=\"val_loss\", direction=\"min\")\n",
      "\n",
      "Trial 08 summary\n",
      "Hyperparameters:\n",
      "units_0: 128\n",
      "activation: sigmoid\n",
      "units_1: 32\n",
      "optimizer: rmsprop\n",
      "learning_rate: 0.01\n",
      "Score: 0.18966706097126007\n",
      "\n",
      "Trial 02 summary\n",
      "Hyperparameters:\n",
      "units_0: 64\n",
      "activation: sigmoid\n",
      "units_1: 96\n",
      "optimizer: rmsprop\n",
      "learning_rate: 0.01\n",
      "Score: 0.19087915122509003\n",
      "\n",
      "Trial 04 summary\n",
      "Hyperparameters:\n",
      "units_0: 32\n",
      "activation: sigmoid\n",
      "units_1: 64\n",
      "optimizer: rmsprop\n",
      "learning_rate: 0.01\n",
      "Score: 0.19434860348701477\n",
      "\n",
      "Trial 09 summary\n",
      "Hyperparameters:\n",
      "units_0: 128\n",
      "activation: tanh\n",
      "units_1: 128\n",
      "optimizer: adam\n",
      "learning_rate: 0.01\n",
      "Score: 0.19737371802330017\n",
      "\n",
      "Trial 00 summary\n",
      "Hyperparameters:\n",
      "units_0: 96\n",
      "activation: tanh\n",
      "units_1: 64\n",
      "optimizer: rmsprop\n",
      "learning_rate: 0.001\n",
      "Score: 0.20036818087100983\n",
      "\n",
      "Trial 07 summary\n",
      "Hyperparameters:\n",
      "units_0: 32\n",
      "activation: tanh\n",
      "units_1: 64\n",
      "optimizer: rmsprop\n",
      "learning_rate: 0.001\n",
      "Score: 0.2092665135860443\n",
      "\n",
      "Trial 06 summary\n",
      "Hyperparameters:\n",
      "units_0: 32\n",
      "activation: tanh\n",
      "units_1: 64\n",
      "optimizer: adagrad\n",
      "learning_rate: 0.01\n",
      "Score: 0.21824941039085388\n",
      "\n",
      "Trial 01 summary\n",
      "Hyperparameters:\n",
      "units_0: 96\n",
      "activation: sigmoid\n",
      "units_1: 64\n",
      "optimizer: rmsprop\n",
      "learning_rate: 0.001\n",
      "Score: 0.22221212089061737\n",
      "\n",
      "Trial 05 summary\n",
      "Hyperparameters:\n",
      "units_0: 64\n",
      "activation: tanh\n",
      "units_1: 128\n",
      "optimizer: adagrad\n",
      "learning_rate: 0.001\n",
      "Score: 0.27966731786727905\n",
      "\n",
      "Trial 03 summary\n",
      "Hyperparameters:\n",
      "units_0: 128\n",
      "activation: sigmoid\n",
      "units_1: 64\n",
      "optimizer: rmsprop\n",
      "learning_rate: 0.0001\n",
      "Score: 0.30269038677215576\n"
     ]
    }
   ],
   "source": [
    "bayesian_tuner.results_summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KKADFrCImWce",
    "outputId": "3108569d-d52d-4bd5-f7e1-c8061c04b7e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results summary\n",
      "Results in hyperband\\customer_churn\n",
      "Showing 10 best trials\n",
      "Objective(name=\"val_loss\", direction=\"min\")\n",
      "\n",
      "Trial 0003 summary\n",
      "Hyperparameters:\n",
      "units_0: 128\n",
      "activation: relu\n",
      "units_1: 64\n",
      "optimizer: rmsprop\n",
      "learning_rate: 0.01\n",
      "tuner/epochs: 3\n",
      "tuner/initial_epoch: 1\n",
      "tuner/bracket: 1\n",
      "tuner/round: 1\n",
      "tuner/trial_id: 0000\n",
      "Score: 0.19221267104148865\n",
      "\n",
      "Trial 0005 summary\n",
      "Hyperparameters:\n",
      "units_0: 32\n",
      "activation: relu\n",
      "units_1: 64\n",
      "optimizer: rmsprop\n",
      "learning_rate: 0.001\n",
      "tuner/epochs: 3\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 0\n",
      "tuner/round: 0\n",
      "Score: 0.19535459578037262\n",
      "\n",
      "Trial 0000 summary\n",
      "Hyperparameters:\n",
      "units_0: 128\n",
      "activation: relu\n",
      "units_1: 64\n",
      "optimizer: rmsprop\n",
      "learning_rate: 0.01\n",
      "tuner/epochs: 1\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 1\n",
      "tuner/round: 0\n",
      "Score: 0.22235043346881866\n",
      "\n",
      "Trial 0004 summary\n",
      "Hyperparameters:\n",
      "units_0: 64\n",
      "activation: tanh\n",
      "units_1: 32\n",
      "optimizer: rmsprop\n",
      "learning_rate: 0.0001\n",
      "tuner/epochs: 3\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 0\n",
      "tuner/round: 0\n",
      "Score: 0.22314521670341492\n",
      "\n",
      "Trial 0001 summary\n",
      "Hyperparameters:\n",
      "units_0: 96\n",
      "activation: tanh\n",
      "units_1: 96\n",
      "optimizer: rmsprop\n",
      "learning_rate: 0.0001\n",
      "tuner/epochs: 1\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 1\n",
      "tuner/round: 0\n",
      "Score: 0.23496785759925842\n",
      "\n",
      "Trial 0002 summary\n",
      "Hyperparameters:\n",
      "units_0: 128\n",
      "activation: relu\n",
      "units_1: 32\n",
      "optimizer: adam\n",
      "learning_rate: 0.0001\n",
      "tuner/epochs: 1\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 1\n",
      "tuner/round: 0\n",
      "Score: 0.2889432907104492\n"
     ]
    }
   ],
   "source": [
    "hyperband_tuner.results_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "###choosing hyperband because faster and similar test loss to bayesian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "Test Accuracy: 0.9144\n"
     ]
    }
   ],
   "source": [
    "best_model = hyperband_tuner.get_best_models(num_models=1)[0]\n",
    "y_pred_prob = best_model.predict(X_test_scaled)  # Get probability outputs\n",
    "y_pred = (y_pred_prob > 0.5).astype(int)  # Convert probabilities to binary labels (0 or 1)\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      " [[6784  223]\n",
      " [ 547 1446]]\n",
      "TP is: 1446\n",
      "TN is: 6784\n",
      "FP is: 223\n",
      "FN is: 547\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "\n",
    "print(\"TP is:\", conf_matrix[1,1])\n",
    "print(\"TN is:\", conf_matrix[0,0])\n",
    "print(\"FP is:\", conf_matrix[0,1])\n",
    "print(\"FN is:\", conf_matrix[1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.97      0.95      7007\n",
      "           1       0.87      0.73      0.79      1993\n",
      "\n",
      "    accuracy                           0.91      9000\n",
      "   macro avg       0.90      0.85      0.87      9000\n",
      "weighted avg       0.91      0.91      0.91      9000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "###logisitc regression\n",
    "\n",
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(X, y, test_size = 0.3, random_state=42)\n",
    "\n",
    "\n",
    "# Apply preprocessing (only fit on training data)\n",
    "X_train_scaled1 = data_transformer.fit_transform(X_train1)  # Fit only on training data \n",
    "X_test_scaled1 = data_transformer.transform(X_test1)        # Transform test data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.16914444  0.0371894  -0.14033906 -0.62910834  1.0058119   1.38698634\n",
      "  -0.0159445  -0.44918338  0.02944378  0.18088065 -1.55347438  0.67255987\n",
      "  -0.78408788  0.00792862 -0.25910475 -0.6595777  -1.14837178  0.00771856\n",
      "  -7.70466907]]\n",
      "[-0.50809809]\n",
      "Test Accuracy is: 0.8938518518518519\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Run the Logistic Regression Model: \n",
    "\n",
    "## (a) Define function, train the model. Report coefficient.\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train_scaled1, y_train1)\n",
    "print(logreg.coef_)\n",
    "print(logreg.intercept_)\n",
    "\n",
    "y_pred1 = logreg.predict(X_test_scaled1)\n",
    "## (b) Evaluation model performance - accuracy\n",
    "print(\"Test Accuracy is:\", logreg.score(X_test_scaled1, y_test1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      " [[9822  671]\n",
      " [ 762 2245]]\n",
      "TP is: 2245\n",
      "TN is: 9822\n",
      "FP is: 671\n",
      "FN is: 762\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.94      0.93     10493\n",
      "           1       0.77      0.75      0.76      3007\n",
      "\n",
      "    accuracy                           0.89     13500\n",
      "   macro avg       0.85      0.84      0.85     13500\n",
      "weighted avg       0.89      0.89      0.89     13500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "conf_matrix = confusion_matrix(y_test1, y_pred1)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "\n",
    "print(\"TP is:\", conf_matrix[1,1])\n",
    "print(\"TN is:\", conf_matrix[0,0])\n",
    "print(\"FP is:\", conf_matrix[0,1])\n",
    "print(\"FN is:\", conf_matrix[1,0])\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(\"Classification Report:\\n\", classification_report(y_test1, y_pred1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
